{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13788,
     "status": "ok",
     "timestamp": 1682144805959,
     "user": {
      "displayName": "Manuel Felipe Porras Tasc√≥n",
      "userId": "13585604089293019409"
     },
     "user_tz": 300
    },
    "id": "XrfhlXkFmSIh",
    "outputId": "1ede95de-aeef-4191-8337-d10ad2e54511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ydata-profiling in c:\\users\\user\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: matplotlib<3.7,>=3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (3.3.4)\n",
      "Requirement already satisfied: multimethod<1.10,>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.4)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (6.0)\n",
      "Requirement already satisfied: visions[type_image_path]==0.7.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.7.5)\n",
      "Requirement already satisfied: typeguard<2.14,>=2.13.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.13.3)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.1.12)\n",
      "Requirement already satisfied: pandas!=1.4.0,<1.6,>1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.2.4)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.11.3)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.12.0)\n",
      "Requirement already satisfied: scipy<1.10,>=1.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.6.2)\n",
      "Requirement already satisfied: seaborn<0.13,>=0.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.11.1)\n",
      "Requirement already satisfied: tqdm<4.65,>=4.48.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.64.1)\n",
      "Requirement already satisfied: statsmodels<0.14,>=0.13.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.13.5)\n",
      "Requirement already satisfied: pydantic<1.11,>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.8.2)\n",
      "Requirement already satisfied: numpy<1.24,>=1.16.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.22.4)\n",
      "Requirement already satisfied: requests<2.29,>=2.24.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.28.1)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (21.4.0)\n",
      "Requirement already satisfied: tangled-up-in-unicode>=0.0.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (0.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (2.8.4)\n",
      "Requirement already satisfied: imagehash in c:\\users\\user\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (4.2.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<3.7,>=3.2->ydata-profiling) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas!=1.4.0,<1.6,>1.1->ydata-profiling) (2022.1)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<1.11,>=1.8.1->ydata-profiling) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<2.29,>=2.24.0->ydata-profiling) (1.26.11)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from statsmodels<0.14,>=0.13.2->ydata-profiling) (21.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm<4.65,>=4.48.2->ydata-profiling) (0.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels<0.14,>=0.13.2->ydata-profiling) (1.16.0)\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\user\\anaconda3\\lib\\site-packages (from imagehash->visions[type_image_path]==0.7.5->ydata-profiling) (1.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\user\\anaconda3\\lib\\site-packages (0.0.52)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
      "Requirement already satisfied: anyascii in c:\\users\\user\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install ydata-profiling\n",
    "!pip install contractions\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5371,
     "status": "ok",
     "timestamp": 1682144811312,
     "user": {
      "displayName": "Manuel Felipe Porras Tasc√≥n",
      "userId": "13585604089293019409"
     },
     "user_tz": 300
    },
    "id": "j1mJjTZXmSyv",
    "outputId": "a14d2d0b-6d4b-4c6e-bf89-435e06c98b9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\user\\anaconda3\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wordcloud) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\user\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1682144811313,
     "user": {
      "displayName": "Manuel Felipe Porras Tasc√≥n",
      "userId": "13585604089293019409"
     },
     "user_tz": 300
    },
    "id": "XGM0HbDMmMrc"
   },
   "outputs": [],
   "source": [
    "#Manejo de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualizaci√≥n de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect # Correctly generate plurals, singular nouns, ordinals, indefinite articles; convert numbers to words.\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#Analisis profundo de datos\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "#Entrenamiento del modelo\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import inflect\n",
    "import re, string, unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1682144811313,
     "user": {
      "displayName": "Manuel Felipe Porras Tasc√≥n",
      "userId": "13585604089293019409"
     },
     "user_tz": 300
    },
    "id": "mPfnN4WqoTGX"
   },
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1682144811746,
     "user": {
      "displayName": "Manuel Felipe Porras Tasc√≥n",
      "userId": "13585604089293019409"
     },
     "user_tz": 300
    },
    "id": "xVA2chBCPH4h",
    "outputId": "f043ad80-dbac-450f-e5b9-a766ddc2c956"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Si est√° buscando una pel√≠cula de guerra t√≠pica...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Supongo que algunos directores de pel√≠culas de...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Es dif√≠cil contarle m√°s sobre esta pel√≠cula si...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>La pel√≠cula comienza muy lentamente, con el es...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Esta pel√≠cula es verdadera acci√≥n en su m√°xima...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          review_es sentimiento\n",
       "0           0  Si est√° buscando una pel√≠cula de guerra t√≠pica...    positivo\n",
       "1           1  Supongo que algunos directores de pel√≠culas de...    positivo\n",
       "2           2  Es dif√≠cil contarle m√°s sobre esta pel√≠cula si...    positivo\n",
       "3           3  La pel√≠cula comienza muy lentamente, con el es...    positivo\n",
       "4           4  Esta pel√≠cula es verdadera acci√≥n en su m√°xima...    positivo"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leer e ignorar columnas sin nombre.\n",
    "df_Big_AHR = pd.read_csv(\"MovieReviews.csv\",  index_col=False)\n",
    "df_Big_AHR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682144811746,
     "user": {
      "displayName": "Manuel Felipe Porras Tasc√≥n",
      "userId": "13585604089293019409"
     },
     "user_tz": 300
    },
    "id": "nfYAekYWk1j0",
    "outputId": "d2d7b208-7c39-4255-a762-8fdccc39e41b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Si est√° buscando una pel√≠cula de guerra t√≠pica...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Supongo que algunos directores de pel√≠culas de...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Es dif√≠cil contarle m√°s sobre esta pel√≠cula si...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La pel√≠cula comienza muy lentamente, con el es...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Esta pel√≠cula es verdadera acci√≥n en su m√°xima...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_es sentimiento\n",
       "0  Si est√° buscando una pel√≠cula de guerra t√≠pica...    positivo\n",
       "1  Supongo que algunos directores de pel√≠culas de...    positivo\n",
       "2  Es dif√≠cil contarle m√°s sobre esta pel√≠cula si...    positivo\n",
       "3  La pel√≠cula comienza muy lentamente, con el es...    positivo\n",
       "4  Esta pel√≠cula es verdadera acci√≥n en su m√°xima...    positivo"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar columna Unnamed: 0\n",
    "df_Big_AHR = df_Big_AHR.drop(['Unnamed: 0'], axis=1)\n",
    "df_Big_AHR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_es</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>¬øQu√© puedo decir al respecto? Es otra pel√≠cula...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Se emiti√≥ en la televisi√≥n ayer, as√≠ que decid...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>Me vio obligado a ver 'Changi' el a√±o pasado e...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Acabo de capturar un episodio sobre Brad, el a...</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>Aqu√≠ est√° lo que es bueno acerca de \"la regla ...</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review_es sentimiento\n",
       "2435  ¬øQu√© puedo decir al respecto? Es otra pel√≠cula...    positivo\n",
       "188   Se emiti√≥ en la televisi√≥n ayer, as√≠ que decid...    positivo\n",
       "3507  Me vio obligado a ver 'Changi' el a√±o pasado e...    negativo\n",
       "124   Acabo de capturar un episodio sobre Brad, el a...    positivo\n",
       "4005  Aqu√≠ est√° lo que es bueno acerca de \"la regla ...    negativo"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df_Big_AHR, test_size=0.2, random_state=33)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2435    ¬øQu√© puedo decir al respecto? Es otra pel√≠cula...\n",
       "188     Se emiti√≥ en la televisi√≥n ayer, as√≠ que decid...\n",
       "3507    Me vio obligado a ver 'Changi' el a√±o pasado e...\n",
       "124     Acabo de capturar un episodio sobre Brad, el a...\n",
       "4005    Aqu√≠ est√° lo que es bueno acerca de \"la regla ...\n",
       "                              ...                        \n",
       "57      Me encantan las pel√≠culas de Michael Landon Jr...\n",
       "3273    The only reason any of the hundred or so users...\n",
       "2706    No malas actuaciones.Whoopi juega el papel sab...\n",
       "578     Para el registro, soy un abanico rizado a trav...\n",
       "2439    \"Der TDESKING\", el segundo pel√≠cula de largome...\n",
       "Name: review_es, Length: 4000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2435    positivo\n",
       "188     positivo\n",
       "3507    negativo\n",
       "124     positivo\n",
       "4005    negativo\n",
       "          ...   \n",
       "57      positivo\n",
       "3273    negativo\n",
       "2706    negativo\n",
       "578     positivo\n",
       "2439    positivo\n",
       "Name: sentimiento, Length: 4000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = train['review_es'], train['sentimiento'] \n",
    "display(X_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1682144811747,
     "user": {
      "displayName": "Manuel Felipe Porras Tasc√≥n",
      "userId": "13585604089293019409"
     },
     "user_tz": 300
    },
    "id": "eNQ52O3unl9Q",
    "outputId": "7de222b9-db6e-4526-8c59-00b7d012f752"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Si est√° buscando una pel√≠cula de guerra t√≠pica...\n",
       "1       Supongo que algunos directores de pel√≠culas de...\n",
       "2       Es dif√≠cil contarle m√°s sobre esta pel√≠cula si...\n",
       "3       La pel√≠cula comienza muy lentamente, con el es...\n",
       "4       Esta pel√≠cula es verdadera acci√≥n en su m√°xima...\n",
       "                              ...                        \n",
       "4995    \"Criminal decente ordinario\" es triste porque ...\n",
       "4996    Savage Island (2003) es una pel√≠cula coja.Es m...\n",
       "4997    Quien escribi√≥ el script para esta pel√≠cula no...\n",
       "4998    in a TV-movie 70's kind of way It's one of tho...\n",
       "4999    Pel√≠cula decepcionante, predecible en la que u...\n",
       "Name: review_es, Length: 5000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       positivo\n",
       "1       positivo\n",
       "2       positivo\n",
       "3       positivo\n",
       "4       positivo\n",
       "          ...   \n",
       "4995    negativo\n",
       "4996    negativo\n",
       "4997    negativo\n",
       "4998    negativo\n",
       "4999    negativo\n",
       "Name: sentimiento, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df_Big_AHR['review_es'], df_Big_AHR['sentimiento'] \n",
    "display(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Convertir todos los caracteres a min√∫sculas\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convierte todos los caracteres de una lista de palabras tokenizadas a min√∫sculas\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# Paso 2: Reemplazar los n√∫meros\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Reemplaza todas las ocurrencias de n√∫meros enteros en una lista de palabras tokenizadas con su representaci√≥n textual\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "# Paso 3: Eliminar la puntuaci√≥n\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Elimina la puntuaci√≥n de una lista de palabras tokenizadas\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# Paso 4: Eliminar caracteres no ASCII\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Elimina los caracteres no ASCII de una lista de palabras tokenizadas\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# Paso 5: Remover stopwords\n",
    "def remove_stopwords(words, stopwords=stopwords.words('spanish')):\n",
    "    \"\"\"Elimina las palabras vac√≠as de una lista de palabras tokenizadas\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "# Paso 6: Aplicar la ra√≠z\n",
    "def stem_words( words):\n",
    "    \"\"\"Palabras clave en la lista de palabras tokenizadas\"\"\"\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "# Paso 7: Aplicar la lematizaci√≥n\n",
    "def lemmatize_verbs( words):\n",
    "    \"\"\"Lematizar verbos en lista de palabras tokenizadas\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# Aplicar la ra√≠z y la lematizaci√≥n: Pasos 6 y 7\n",
    "def stem_and_lemmatize( words):\n",
    "    words = stem_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "# Preprocesamiento: Pasos 1-5\n",
    "def preproccesing(words):\n",
    "    words = to_lowercase(words) # 1\n",
    "    words = replace_numbers(words) # 2\n",
    "    words = remove_punctuation(words) # 3\n",
    "    words = remove_non_ascii(words) # 4    \n",
    "    words = remove_stopwords(words) # 5\n",
    "    return words\n",
    "\n",
    "\n",
    "# new_X_train = pd.Series(X)\n",
    "# new_X_train = new_X_train.apply(contractions.fix) # Corregir las contracciones\n",
    "# new_X_train = new_X_train.apply(word_tokenize) #  Tokenizar\n",
    "# new_X_train = new_X_train.apply(lambda x: preproccesing(x)) # Preprocesamiento\n",
    "# new_X_train = new_X_train.apply(lambda x: stem_and_lemmatize(x)) # Aplicar la ra√≠z y la lematizaci√≥n\n",
    "# new_X_train = new_X_train.apply(lambda x: ' '.join(map(str, x))) # Convertir la lista de palabras en una cadena de texto\n",
    "\n",
    "\n",
    "contra=FunctionTransformer(contractions.fix)\n",
    "t1= FunctionTransformer(word_tokenize)\n",
    "p1= FunctionTransformer(lambda x: preproccesing(x))\n",
    "stem_lem= FunctionTransformer(lambda x: stem_and_lemmatize(x))\n",
    "list_words= FunctionTransformer(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "contract= ColumnTransformer(transformers=[['contractions',contra,list(range(3))]],remainder='passthrough')\n",
    "tokenizer= ColumnTransformer(transformers=[['tokenizer',t1,list(range(3))]],remainder='passthrough')\n",
    "preprocessor= ColumnTransformer(transformers=[['preprocessor',p1,list(range(3))]],remainder='passthrough')\n",
    "lemm=ColumnTransformer(transformers=[['stem_lem',stem_lem,list(range(3))]],remainder='passthrough')\n",
    "stem=ColumnTransformer(transformers=[['list_words',list_words,list(range(3))]],remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def remove_non_ascii(words):\n",
    "#     \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def to_lowercase(words):\n",
    "#     \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = word.lower()\n",
    "#         new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_punctuation(words):\n",
    "#     \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "#         if new_word != '':\n",
    "#             new_words.append(new_word)\n",
    "#     return new_words\n",
    "\n",
    "# def replace_numbers(words):\n",
    "#     \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "#     p = inflect.engine()\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         if word.isdigit():\n",
    "#             new_word = p.number_to_words(word)\n",
    "#             new_words.append(new_word)\n",
    "#         else:\n",
    "#             new_words.append(word)\n",
    "#     return new_words\n",
    "\n",
    "# def remove_stopwords(words):\n",
    "#     \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         if not word in stopwords.words():\n",
    "#             new_words.append(word)\n",
    "#     return new_words\n",
    "\n",
    "# def preprocessing(words):\n",
    "#     words = to_lowercase(words)\n",
    "# #     words = replace_numbers(words)\n",
    "#     words = remove_punctuation(words)\n",
    "#     words = remove_non_ascii(words)\n",
    "#     words = remove_stopwords(words)\n",
    "#     return words\n",
    "\n",
    "# ps = LancasterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def stem_words(words):\n",
    "#     \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_words.append(ps.stem(word))\n",
    "#     return new_words\n",
    "\n",
    "# def lemmatize_verbs(words):\n",
    "#     \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "#     new_words = []\n",
    "#     for word in words:\n",
    "#         new_words.append(lemmatizer.lemmatize(word))\n",
    "#     return new_words\n",
    "\n",
    "# def stem_and_lemmatize(words):\n",
    "#     words = stem_words(words)\n",
    "#     words = lemmatize_verbs(words)\n",
    "#     return words\n",
    "\n",
    "# t1 = FunctionTransformer(word_tokenize)\n",
    "# p1 = FunctionTransformer(preprocessing)\n",
    "# l1= FunctionTransformer(lemmatize_verbs)\n",
    "# s1=FunctionTransformer(stem_and_lemmatize)\n",
    "# tokenizer= ColumnTransformer(transformers=[['tokenizer',t1,list(range(3))]],remainder='passthrough')\n",
    "# preprocessor= ColumnTransformer(transformers=[['preprocessor',p1,list(range(3))]],remainder='passthrough')\n",
    "# lemm=ColumnTransformer(transformers=[['lemm',l1,list(range(3))]],remainder='passthrough')\n",
    "# stem=ColumnTransformer(transformers=[['stem',s1,list(range(3))]],remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSFORMADOR\n",
    "selected_cols=[\"review_es\"]\n",
    "\n",
    "words_transformer = Pipeline(\n",
    "    \n",
    "    steps=[(\"contractions\",contra),(\"tokenizer\",tokenizer),('preprocessor',preprocessor),(\"stem_lem\", stem_lem),(\"list_words\", list_words), (\"vect\", CountVectorizer(lowercase=False))]\n",
    "\n",
    ")\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"review_es\", words_transformer, selected_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('review_es',\n",
       "                                                  Pipeline(steps=[('contractions',\n",
       "                                                                   FunctionTransformer(func=<function fix at 0x000001EDB4530F70>)),\n",
       "                                                                  ('tokenizer',\n",
       "                                                                   ColumnTransformer(remainder='passthrough',\n",
       "                                                                                     transformers=[['tokenizer',\n",
       "                                                                                                    FunctionTransformer(func=<function word_tokenize at 0x000001EDB09150D0>),\n",
       "                                                                                                    [0,\n",
       "                                                                                                     1,\n",
       "                                                                                                     2]]])),\n",
       "                                                                  ('preprocessor',...\n",
       "                                                                                                    FunctionTransformer(func=<function <lambda> at 0x000001EDB7261CA0>),\n",
       "                                                                                                    [0,\n",
       "                                                                                                     1,\n",
       "                                                                                                     2]]])),\n",
       "                                                                  ('stem_lem',\n",
       "                                                                   FunctionTransformer(func=<function <lambda> at 0x000001EDB72615E0>)),\n",
       "                                                                  ('list_words',\n",
       "                                                                   FunctionTransformer(func=<function <lambda> at 0x000001EDB72618B0>)),\n",
       "                                                                  ('vect',\n",
       "                                                                   CountVectorizer(lowercase=False))]),\n",
       "                                                  ['review_es'])])),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(max_depth=50, max_features='sqrt',\n",
       "                                        n_estimators=1000, random_state=0))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODELO\n",
    "model = ('model', RandomForestClassifier(random_state=0, max_depth=50,max_features=\"sqrt\",n_estimators=1000))\n",
    "# PIPELINE\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", pre), model])\n",
    "\n",
    "X, y = pd.DataFrame(df_Big_AHR['review_es']),df_Big_AHR['sentimiento']\n",
    "y = (y == 'positivo').astype(int)\n",
    "\n",
    "#pipeline.fit(X,y)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data= df_Big_AHR['review_es']\n",
    "new_X_train = pd.Series(X_data)\n",
    "new_X_train = new_X_train.apply(contractions.fix) # Corregir las contracciones\n",
    "new_X_train = new_X_train.apply(word_tokenize) #  Tokenizar\n",
    "new_X_train = new_X_train.apply(lambda x: preproccesing(x)) # Preprocesamiento\n",
    "new_X_train = new_X_train.apply(lambda x: stem_and_lemmatize(x)) # Aplicar la ra√≠z y la lematizaci√≥n\n",
    "new_X_train = new_X_train.apply(lambda x: ' '.join(map(str, x))) # Convertir la lista de palabras en una cadena de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=False) \n",
    "vectors = vectorizer.fit_transform(X_data)\n",
    "copia=vectors\n",
    "copia=pd.DataFrame.sparse.from_spmatrix(copia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('model',\n",
       "                 RandomForestClassifier(max_depth=50, max_features='sqrt',\n",
       "                                        n_estimators=1000, random_state=0))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2 = Pipeline(steps=[model])\n",
    "\n",
    "pipeline2.fit(copia,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train_SVM = pipeline2.predict(copia)\n",
    "y_pred_train_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Report for Support Vector Machines\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      2500\n",
      "           1       0.99      1.00      1.00      2500\n",
      "\n",
      "    accuracy                           1.00      5000\n",
      "   macro avg       1.00      1.00      1.00      5000\n",
      "weighted avg       1.00      1.00      1.00      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_SVC = classification_report(y, y_pred_train_SVM)\n",
    "\n",
    "print(\"Train Report for Support Vector Machines\\n\", report_SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2.score(copia,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelo.joblib']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'modelo.joblib'\n",
    "dump(pipeline2, filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para predecir\n",
    "pipeline2.predict(np.array(copia.iloc[-1].tolist()).reshape(1,-1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(copia.iloc[-1].tolist()).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Savage Island (2003) es una pel√≠cula coja.Es m√°s como un video de video en casa con iluminaci√≥n muy m√≠nima y una actuaci√≥n horrible.No solo que la historia y el script fueron desgraciados.No s√© por qu√© se hizo esta pel√≠cula.He visto muchos movimientos en mi tiempo y los que realmente odio son las pel√≠culas que me hacen enojar.Este hizo hervir mi sangre.Las situaciones fueron inanidas en el mejor de los casos.Si hice una pel√≠cula como esta, hubiera sido un corto.Realmente porque esos \"idjits\" de la revestimiento \"no habr√≠an estado en la foto. No te dejes enga√±ar por la portada en el D.V.D.Soy un √°vido observador de mal cine.Pero esta pel√≠cula es virtualmente inconsciente.No me importa que las pel√≠culas se filmen en D.V.Pero si vas a hacer eso, haga que la pel√≠cula sea agradable, no un recaudado cansado de las pel√≠culas de terror superiores (sans turn turn). Tengo que no recomendar este desperdicio de disco.Si te encuentras con este en la tienda de alquiler, pase a.movies que hacen que el suyo est√© realmente enojado, obtenga una autom√°tica 1.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Big_AHR['review_es'].iloc[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2919900869.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [134]\u001b[1;36m\u001b[0m\n\u001b[1;33m    -4 y -13 , -15 , -19,-20,-26, -28\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# -4 y -13 , -15 , -19,-20,-26, -28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para predecir 2\n",
    "linea= df_Big_AHR['review_es'].iloc[-28] \n",
    "words = pd.Series(linea)\n",
    "words = words.apply(contractions.fix) # Corregir las contracciones\n",
    "words = words.apply(word_tokenize) #  Tokenizar\n",
    "words = words.apply(lambda x: preproccesing(x)) # Preprocesamiento\n",
    "words = words.apply(lambda x: stem_and_lemmatize(x)) # Aplicar la ra√≠z y la lematizaci√≥n\n",
    "words = words.apply(lambda x: ' '.join(map(str, x))) # Convertir la lista de palabras en una cadena de texto\n",
    "vec = vectorizer.transform(words)\n",
    "pipeline2.predict(vec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok, ser√© breve.Esta pel√≠cula no era simplemente mala, era muy malo, con Line4s como \"Si tratas con el diablo, espera obtener sh * t en tus zapatos\", sabes que est√°s en territorio cinematogr√°fico horrible.Despu√©s de ver esta pel√≠cula, quer√≠a matarme a m√≠ y a toda mi familia, me dio una gran sensaci√≥n de odio a s√≠ mismo, quer√≠a hacer un asesinato.No mires esta pel√≠cula.Voy a matar de nuevo.Pero cuando lo hago, ser√° terriblemente editado con una pat√©tica banda sonora y un disparo de stock para contratar secuencias de acci√≥n y malas camisas. Enfortunadamente, hay un fleatorio, es la primera pel√≠cula de acci√≥n para presentar una Ford Sierra de tres puertas. Confortunadamente, termina con desgaste en suTecho: (PS: peor que el marcador'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Big_AHR['review_es'].iloc[-28] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para predecir 2\n",
    "linea= df_Big_AHR['review_es'].iloc[1] \n",
    "words=word_tokenize(linea)\n",
    "words=preproccesing(words)\n",
    "words=stem_and_lemmatize(words)\n",
    "words= ' '.join(map(str, words))\n",
    "vec = vectorizer.transform([words])\n",
    "pipeline2.predict(vec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supongo que algunos directores de pel√≠culas de lujo estaban sentados buscando su abrigo, Grappa, Aramangac o jugo de selva alguna noche en los a√±os 80 durante los cannes u otro festival de cine y uno dijo: \"Oye, chicos, hagamos una pel√≠cula donde cada uno de nosotros cree un segmento alrededor una clase mundial aria \". Welllll ... Tipo de tipo de trabajo. Claramente, alguien fue lo suficientemente inteligente como para seleccionar algunas de las mejores grabaciones de las Arias elegidas, por ejemplo, Nessun Dorma de Bjoreling, as√≠ que si estuvieras ciego y mintiendo en el piso, simplemente escuchando el DVD, obtuviste m√°s que el valor de tu dinero. No todos los directores lo sucedieron, pero m√°s lo hicieron, lo que no y la pel√≠cula parece mejorar con cada visi√≥n a lo largo de los a√±os. Mi favorito es el d√∫o de amor inquietantemente hermoso de Die Tod Statd; Est√° bien, un joven desnudo, Elizabeth Hurley es ojos, pero su marido cant√≥ a ella, el fantasma de su esposa, es incre√≠blemente hermoso con la m√∫sica del amor segundo solo para \"Gia Nella Notte Densa\" de Otello y Desdemona en todo el repertorio oper√≠stico. ¬øPodr√≠a haber sido mejor el flick, seguro, lo que no podr√≠a haber sido, pero vale la pena una visi√≥n especialmente de un estado de √°nimo hiper-rom√°ntico?'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Big_AHR['review_es'].iloc[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyEXbd4+wMTi5rP/Zoe37r",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
